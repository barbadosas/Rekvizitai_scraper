{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "b8fd0e31f0b2a7318297d5db825c9ab9dd9261554b16432e8fed70c433ada44c"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://rekvizitai.vz.lt/imone/maxima_lt_uab/atsiliepimai/' #change url for required company\n",
    "file_name = 'Maxima_results.csv' #change for different name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pages():\n",
    "    pages_list = []\n",
    "    get_url = requests.get(url)\n",
    "    soup = BeautifulSoup(get_url.content, \"html.parser\")\n",
    "    for data in soup.findAll('div', {\"class\": \"pager\"}):\n",
    "        for x in data.findAll('a'):\n",
    "            pages_list.append(x.text)\n",
    "    return pages_list[-2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "def get_urls(pages):\n",
    "    for i in range(1, pages):\n",
    "        urls.append(url + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name_comment(soup, link):\n",
    "    name_list = []\n",
    "    comment_list = []\n",
    "    date_list = []\n",
    "    ip_list = []\n",
    "    link_list = []\n",
    "    \n",
    "\n",
    "    for div in soup.find_all(\"div\", {'class':'quote'}): \n",
    "        div.decompose() # remove quote divs\n",
    "     \n",
    "    for data in soup.findAll('div', {\"class\": \"comment\"}):\n",
    "        for comments in data.findAll('div', {\"class\": \"text\"}):\n",
    "            comments_striped = comments.text.strip()\n",
    "            comment_list.append(comments_striped)\n",
    "            link_list.append(link) #count how many links and add them to find comments if needed later\n",
    "    for data in soup.findAll('div', {\"class\": \"floatLeft\"}):\n",
    "        for name in data.findAll('strong'):\n",
    "            name_list.append(name.string)\n",
    "\n",
    "    for data in soup.findAll('a', {\"class\": \"date\"}):\n",
    "        for data_split in data:\n",
    "            if data_split.startswith('20'):\n",
    "                date_list.append(data_split)\n",
    "            if data_split.startswith('IP'):\n",
    "                splited = data_split.split()\n",
    "                ip_list.append(splited[1])            \n",
    "    \n",
    "    df = pd.DataFrame(list(zip(date_list, ip_list, name_list, comment_list, link_list )))\n",
    "    df.columns = [\"Date\", \"Ip\", \"Name\", \"Comment\", \"Link\"]\n",
    "\n",
    "    if not os.path.isfile(file_name): # If file exist - headers are not saved, file is appended\n",
    "        df.to_csv(file_name, mode='a', header=True, index=False, encoding=\"utf-8-sig\")\n",
    "    else:    # if file exists - headers are not inlucded and file is appended                              \n",
    "        df.to_csv(file_name, mode='a', header=False, index=False, encoding=\"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data():\n",
    "    pages = get_pages()\n",
    "    get_urls(int(pages))\n",
    "    for link in urls:\n",
    "        get_soup = requests.get(link)\n",
    "        soup = BeautifulSoup(get_soup.content, \"html.parser\")\n",
    "        get_name_comment(soup, link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_data()"
   ]
  }
 ]
}